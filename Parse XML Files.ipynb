{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting albumentations\n",
      "  Using cached albumentations-1.4.7-py3-none-any.whl.metadata (37 kB)\n",
      "Requirement already satisfied: numpy>=1.24.4 in f:\\tutorial\\learn-tensorflow-object-detection\\.venv\\lib\\site-packages (from albumentations) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.10.0 in f:\\tutorial\\learn-tensorflow-object-detection\\.venv\\lib\\site-packages (from albumentations) (1.13.0)\n",
      "Collecting scikit-image>=0.21.0 (from albumentations)\n",
      "  Using cached scikit_image-0.23.2-cp310-cp310-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: PyYAML in f:\\tutorial\\learn-tensorflow-object-detection\\.venv\\lib\\site-packages (from albumentations) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in f:\\tutorial\\learn-tensorflow-object-detection\\.venv\\lib\\site-packages (from albumentations) (4.11.0)\n",
      "Collecting scikit-learn>=1.3.2 (from albumentations)\n",
      "  Using cached scikit_learn-1.4.2-cp310-cp310-win_amd64.whl.metadata (11 kB)\n",
      "Collecting pydantic>=2.7.0 (from albumentations)\n",
      "  Using cached pydantic-2.7.1-py3-none-any.whl.metadata (107 kB)\n",
      "Collecting opencv-python-headless>=4.9.0 (from albumentations)\n",
      "  Using cached opencv_python_headless-4.9.0.80-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic>=2.7.0->albumentations)\n",
      "  Using cached annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in f:\\tutorial\\learn-tensorflow-object-detection\\.venv\\lib\\site-packages (from pydantic>=2.7.0->albumentations) (2.18.2)\n",
      "Collecting networkx>=2.8 (from scikit-image>=0.21.0->albumentations)\n",
      "  Using cached networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: pillow>=9.1 in f:\\tutorial\\learn-tensorflow-object-detection\\.venv\\lib\\site-packages (from scikit-image>=0.21.0->albumentations) (10.3.0)\n",
      "Collecting imageio>=2.33 (from scikit-image>=0.21.0->albumentations)\n",
      "  Using cached imageio-2.34.1-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in f:\\tutorial\\learn-tensorflow-object-detection\\.venv\\lib\\site-packages (from scikit-image>=0.21.0->albumentations) (2024.5.10)\n",
      "Requirement already satisfied: packaging>=21 in f:\\tutorial\\learn-tensorflow-object-detection\\.venv\\lib\\site-packages (from scikit-image>=0.21.0->albumentations) (24.0)\n",
      "Collecting lazy-loader>=0.4 (from scikit-image>=0.21.0->albumentations)\n",
      "  Using cached lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn>=1.3.2->albumentations)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in f:\\tutorial\\learn-tensorflow-object-detection\\.venv\\lib\\site-packages (from scikit-learn>=1.3.2->albumentations) (3.5.0)\n",
      "Using cached albumentations-1.4.7-py3-none-any.whl (155 kB)\n",
      "Using cached opencv_python_headless-4.9.0.80-cp37-abi3-win_amd64.whl (38.5 MB)\n",
      "Using cached pydantic-2.7.1-py3-none-any.whl (409 kB)\n",
      "Using cached scikit_image-0.23.2-cp310-cp310-win_amd64.whl (12.7 MB)\n",
      "Using cached scikit_learn-1.4.2-cp310-cp310-win_amd64.whl (10.6 MB)\n",
      "Using cached annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Using cached imageio-2.34.1-py3-none-any.whl (313 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Using cached networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "Installing collected packages: opencv-python-headless, networkx, lazy-loader, joblib, imageio, annotated-types, scikit-learn, scikit-image, pydantic, albumentations\n",
      "Successfully installed albumentations-1.4.7 annotated-types-0.6.0 imageio-2.34.1 joblib-1.4.2 lazy-loader-0.4 networkx-3.3 opencv-python-headless-4.9.0.80 pydantic-2.7.1 scikit-image-0.23.2 scikit-learn-1.4.2\n"
     ]
    }
   ],
   "source": [
    "!pip install albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-2.3.0-cp310-cp310-win_amd64.whl.metadata (26 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached filelock-3.14.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in f:\\tutorial\\learn-tensorflow-object-detection\\.venv\\lib\\site-packages (from torch) (4.11.0)\n",
      "Collecting sympy (from torch)\n",
      "  Using cached sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in f:\\tutorial\\learn-tensorflow-object-detection\\.venv\\lib\\site-packages (from torch) (3.3)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Using cached fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting mkl<=2021.4.0,>=2021.1.1 (from torch)\n",
      "  Using cached mkl-2021.4.0-py2.py3-none-win_amd64.whl.metadata (1.4 kB)\n",
      "Collecting intel-openmp==2021.* (from mkl<=2021.4.0,>=2021.1.1->torch)\n",
      "  Using cached intel_openmp-2021.4.0-py2.py3-none-win_amd64.whl.metadata (1.2 kB)\n",
      "Collecting tbb==2021.* (from mkl<=2021.4.0,>=2021.1.1->torch)\n",
      "  Using cached tbb-2021.12.0-py3-none-win_amd64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in f:\\tutorial\\learn-tensorflow-object-detection\\.venv\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Collecting mpmath>=0.19 (from sympy->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Using cached torch-2.3.0-cp310-cp310-win_amd64.whl (159.8 MB)\n",
      "Using cached mkl-2021.4.0-py2.py3-none-win_amd64.whl (228.5 MB)\n",
      "Using cached intel_openmp-2021.4.0-py2.py3-none-win_amd64.whl (3.5 MB)\n",
      "Using cached tbb-2021.12.0-py3-none-win_amd64.whl (286 kB)\n",
      "Using cached filelock-3.14.0-py3-none-any.whl (12 kB)\n",
      "Using cached fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
      "Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: tbb, mpmath, intel-openmp, sympy, mkl, jinja2, fsspec, filelock, torch\n",
      "Successfully installed filelock-3.14.0 fsspec-2024.5.0 intel-openmp-2021.4.0 jinja2-3.1.4 mkl-2021.4.0 mpmath-1.3.0 sympy-1.12 tbb-2021.12.0 torch-2.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse XML files and extract bounding boxes\n",
    "def parse_voc_xml(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    boxes = []\n",
    "    for member in root.findall(\"object\"):\n",
    "        xmin = int(member.find(\"bndbox\").find(\"xmin\").text)\n",
    "        ymin = int(member.find(\"bndbox\").find(\"ymin\").text)\n",
    "        xmax = int(member.find(\"bndbox\").find(\"xmax\").text)\n",
    "        ymax = int(member.find(\"bndbox\").find(\"ymax\").text)\n",
    "        boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "    return boxes, tree\n",
    "\n",
    "\n",
    "# Define the augmentation pipeline\n",
    "transform = A.Compose(\n",
    "    [\n",
    "        A.Resize(height=350, width=350),\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(format=\"pascal_voc\", label_fields=[\"class_labels\"]),\n",
    ")\n",
    "\n",
    "\n",
    "# Function to process a single image and its XML annotation\n",
    "def process_image(image_path, xml_path):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Parse the XML file to get bounding boxes\n",
    "    boxes, tree = parse_voc_xml(xml_path)\n",
    "    class_labels = [0] * len(boxes)  # Replace with actual class labels if available\n",
    "\n",
    "    # Apply the augmentation\n",
    "    transformed = transform(image=image, bboxes=boxes, class_labels=class_labels)\n",
    "\n",
    "    transformed_image = transformed[\"image\"]\n",
    "    transformed_bboxes = transformed[\"bboxes\"]\n",
    "\n",
    "    # Save the augmented image and create a new XML file with the updated bounding boxes\n",
    "    augmented_image_path = image_path.replace(\"images\", \"augmented_images\")\n",
    "    augmented_xml_path = xml_path.replace(\"images\", \"augmented_images\")\n",
    "\n",
    "    os.makedirs(os.path.dirname(augmented_image_path), exist_ok=True)\n",
    "\n",
    "    cv2.imwrite(augmented_image_path, transformed_image)\n",
    "\n",
    "    create_augmented_xml(augmented_xml_path, transformed_bboxes, tree)\n",
    "\n",
    "\n",
    "# Function to create a new XML file with updated bounding boxes\n",
    "def create_augmented_xml(xml_path, bboxes, original_tree):\n",
    "    root = original_tree.getroot()\n",
    "\n",
    "    for i, member in enumerate(root.findall(\"object\")):\n",
    "        bbox = member.find(\"bndbox\")\n",
    "        bbox.find(\"xmin\").text = str(bboxes[i][0])\n",
    "        bbox.find(\"ymin\").text = str(bboxes[i][1])\n",
    "        bbox.find(\"xmax\").text = str(bboxes[i][2])\n",
    "        bbox.find(\"ymax\").text = str(bboxes[i][3])\n",
    "\n",
    "    original_tree.write(xml_path)\n",
    "\n",
    "\n",
    "# Traverse directories and process images\n",
    "base_dir = \"Tensorflow/workspace/images/collectedimages\"\n",
    "for root, dirs, files in os.walk(base_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".jpg\"):\n",
    "            image_path = os.path.join(root, file)\n",
    "            xml_path = image_path.replace(\".jpg\", \".xml\")\n",
    "\n",
    "            if os.path.exists(xml_path):\n",
    "                process_image(image_path, xml_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
